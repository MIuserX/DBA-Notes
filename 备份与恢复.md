# 备份与恢复

## 本 wiki 的有效性

本 wiki 中有部分内容是围绕 pitr_bakcup.sh 脚本展开的，这些内容的有效性依赖于 pitr_bakcup.sh 脚本的版本。

pitr_backup.sh 这脚本所属项目的 git 地址是：gitlab.corp.qunar.com:pgdba/salt.git

本 wiki 参照的 pitr_backup.sh 版本的 md5sum 是：62c051c544b3c90828127bb6021246ba





## 本 wiki 的目的

本 wiki 的目的是：

- 帮助新同学熟悉 Qunar PostgreSQL DB Cluster 的 DB 备份机制
- 加深对 Qunar PostgreSQL DB Cluster DB 备份机制的理解
- DB 备份出现问题时，作为一个辅助工具快速定位解决问题



## 备份任务的生命周期

只要一套 DB Cluster 还存在，附属于其的备份任务就会一直存在。

以每一天为粒度来看，备份任务



## 备份任务概述

线上大部分 PostgreSQL DB Cluster 的架构都是 一主一从 或 一主两从，还有几套一主三从，少量 PostgreSQL DB Cluster 是单点(姑且把单点也看成DB Cluster吧，以便后面的阐述)。

 

可以到如下数据库中查看当前 Qunar 所有被 PGDBA 管理的 PostgreSQL DB Cluster 的信息：





目前，不是所有 DB Cluster 都被部署了DB备份任务(有些不重要，有些则是因为历史原因)。



进行DB备份任务的 DB Cluster 都会在某个从库机器上部署 DB 备份任务。



## 大概步骤

1）主库用 archive_command 持续将 WAL 归档文件发送到从库所在的 Server 上

2）部署在从库所在 Server 上的DB备份任务脚本每天执行以下 3 步:

2.1）制作当天的DB基础备份

2.2）将昨天的DB基础备份与WAL打包并加密

2.3）将加密的包发送到远端存储集群



## DB备份的内容

先来看一个例子：

```
# 
```



## DB备份存放的位置

DB备份在从库所在 Server 上被制作出来之后，会往远端存储集群发送一份。

本地的一般保留 7 天，远端存储集群的一般会永久保留。



## DB备份任务部署方式

DB备份任务是通过 saltstack 来部署的。

saltstack 是一个集群管理工具，分为 Master 端(控制端)和 Minion 端(被控制端)。

我的理解，saltstack 就是一个可以远程执行命令的工具，并且可以同时多个机器执行命令。

当然，saltstack 还能做更多的事，但这不是本wiki的主题，感兴趣可以自己学习。



目前，saltstack 的 Master 端被部署在 l-pgdata3.vc.cn5 和 l-pgdba2.vc.cn2 上。

这俩 Master 是平级的，这俩的信息不会自动同步。

下面是一个 saltstack 对多个机器远程执行命令的例子：



实际工作过程中，我们只需要记住部署命令就行。

下面就是部署DB备份任务的命令：



## 备份脚本

------

每一套 DB 集群的备份任务都是用 saltstack





## 主库如何将 wal 发送到从库？

在主库的配置文件 postgresql.conf 中配置 archive_command，使用 omnipitr-archive 工具。

一般 archive_command 配置如下：

```
### 为清晰起见将命令多行表示 ``##``/opt/omnipitr/bin/omnipitr-archive``  ``-l ``/export/omnipitr/log/omnipitr-``^Y^m^d.log             ``# omnipitr-archive 的日志文件``  ``-s ``/export/omnipitr/state`                      `# ``  ``-dr ``gzip``=``rsync``:``//l-djb2cdb5``.vc.cn6.qunar.com``/djb2c3_xlog_archive/`  `# WAL将被发送到的位置``  ``-db ``/export/omnipitr/dstbackup``  ``--pid-``file` `/export/omnipitr/omnipitr``.pid``  ``-t ``/export/omnipitr/tmp``  ``-``v` `"%p"
```

omnipitr-archive 实际上用 rsync 命令将 WAL 发送到目标位置的，

因此，从库所在 Server 上必须开启 rsync 服务，并且把主库加入白名单。



实际上，从库上的 rsync 服务是通过 xinetd 工具管理的。

xinetd

这样，部署了DB备份任务的 Server 上实际上看不到一个以 Daemon 模式运行的 rsync 服务。

只有在需要 rsync 服务的时候，才能看到。







## 从库 Server 上如何制作基础备份？

从库所在的 Server 上会部署一个备份任务（crontab任务），一般部署在 postgres 用户下：

```
# 下列的 pitr_backup.sh 是备份脚本，djb2c3_config.sh 是配置文件。``# 配置文件的名字以所属的DB集群的业务线号开头，不同的DB集群配置文件的名字不一样。``#` `[postgres@l-djb2cdb5.vc.cn6 ~]$ ``crontab` `-l``# Lines below here are managed by Salt, do not edit``MAILTO=pg-alert@qunar.com``# SALT_CRON_IDENTIFIER:pitr_backup_djb2c3_cron``30 1 * * * ``bash` `/home/q/pgdba/scripts/pitr_backup``.sh ``/home/q/pgdba/conf/djb2c3_config``.sh 1>> ``/home/q/pgdba/log/pitr_backup_djb2c3``.log` `[postgres@l-djb2cdb5.vc.cn6 ~]$
```



pitr_backup.sh 这个脚本会完成以下 3 步：

(1) 制作今天的DB基础备份

(2) 将昨天的DB基础备份与WAL文件打成 tar 包并加密

(3) 将加密的 tar 包发送到远程存储



先来说说制作基础备份，我们使用的是 omnipitr-backup-slave 脚本完成这个步骤。

```
# 下面这个``#``# 这个函数运行的结果是，在 /export/XXX_xlog_archive 目录下生成 3 个文件：``#``#` `base_backup_slave() {``  ``mkdir` `-p $xlog_dir/$dt_today/` `  ``$pitr_bin_path``/omnipitr-backup-slave` `-D $pg_data_dir \``    ``--call-master -d template1 -h $master_host -U $master_user -P $master_port \``    ``-s ``gzip``=$xlog_dir``/realtime` `\``    ``-dl ``gzip``=$xlog_dir/$dt_today \``    ``--log $pitr_dir``/log/base-backup``.log \``    ``--pid-``file` `$pitr_dir``/omnipitr-backup-slave``.pid \``    ``--temp-``dir` `$pitr_dir``/tmp` `\``    ``--psql-path $pg_bin_path``/psql` `\``    ``--pgcontroldata-path $pg_bin_path``/pg_controldata` `\``    ``--verbose` `  ``cc $? ``"base backup failed"``}
```



基础备份制作出来后，接下来是压缩和加密。

压缩和加密的函数如下：

```
# ``#
```



完成压缩与加密后，接下来是把生成的加密文件和密钥发送到远程存储集群。

我们用 rsync 来完成这个步骤：

```
#``#
```



## 备份任务对磁盘空间造成的压力

与制作DB备份相关的文件一般都放在 DB 集群某个从库所在机器的：

/export/XXX_xlog_arichive

目录下(XXX是DB集群的业务线号，不同DB集群业务线号不同)。



该目录下一般有以下几个文件：

(1) 历史的DB备份文件 (本地默认保留最近7天的DB备份文件)

(2) 当天的DB基础备份 (当天的备份任务运行之后会产生该文件)

(3) 当天的 WAL 归档文件 ()



每天备份任务运行一次之后，会产生 5 个文件：昨天的备份文件tar包和加密 key、今天基础备份的3个文件。



举个例子：

```
[root``@l``-djb2cdb2.vc.cn6 /export/djb2c_xlog_archive]# ll``total ``323914232``-rw-r--r-- ``1` `postgres postgres     ``256` `Jun ``25` `17``:``23` `2019``-``06``-``24``.key  # 昨天的备份文件的加密key``-rw-r--r-- ``1` `postgres postgres ``331684741152` `Jun ``25` `17``:``48` `2019``-``06``-``24``.tar.e # 昨天的基础备份+WAL打包加密后的文件``drwxr-xr-x ``2` `postgres postgres     ``4096` `Jun ``25` `17``:``23` `2019``-``06``-``25`    `# 存放当天的DB基础备份的目录``drwxr-xr-x ``2` `postgres postgres   ``3325952` `Jun ``25` `20``:``01` `realtime``[root``@l``-djb2cdb2.vc.cn6 /export/djb2c_xlog_archive]# ll ``2019``-``06``-``25``total ``257833028``-rw-r--r-- ``1` `postgres postgres ``216873179161` `Jun ``25` `14``:``51` `l-djb2cdb2.vc.cn6-data-``2019``-``06``-``25``.tar.gz # 今天的备份文件的目录``-rw-r--r-- ``1` `postgres postgres     ``103` `Jun ``25` `17``:``23` `l-djb2cdb2.vc.cn6-meta-``2019``-``06``-``25``.tar.gz # 今天的备份文件的目录``-rw-r--r-- ``1` `postgres postgres ``47147751773` `Jun ``25` `17``:``23` `l-djb2cdb2.vc.cn6-xlog-``2019``-``06``-``25``.tar.gz # 今天的备份文件的目录
```



一般情况下，每天的 DB 备份任务未运行时，/export/XXX_xlog_archive 目录下会存在：

(1) 从前天开始往前 6 天的 DB 备份文件 (例如今天是 06-28，那么存在 06-21 ～ 06-26 的DB备份文件)

(2) 昨天的基本备份

(3) 昨天 0 点到当前的 WAL 归档文件



DB 备份任务会做如下操作：

(1) 生成今天的基础备份

(2) 将昨天的基础备份 + 昨天 0 点到昨天 24:00 的 WAL 归档文件打包并加密，发送到存储集群存储

(3) 删除 7 天前的 DB 备份文件 (例如今天是 06-28，那么删除 06-21 的DB备份文件)

之后，/export/XXX_xlog_archive 目录下会存在：

(1) 从昨天开始往前 6 天的 DB 备份文件 (例如今天是 06-28，那么存在 06-22 ～ 06-27 的DB备份文件)

(2) 今天的基础备份

(3) 从今天 0 点到当前的 WAL 归档文件



## 参考

omnipitr 工具



xinetd



rsync



## 备份恢复操作步骤



一般登录到对应的 backup_tmp 目录下，会发现以下几个文件：



其中 restore.sh 文件中记录了整个备份恢复测试的前 3 步操作步骤及每步用到的命令：

```shell
#### 设置 rsync 传输密码 
export RSYNC_PASSWORD='*************' 

#### step 1: 从 ops 的 MFS 备份机器上把 DB备份 取下来，并截图 
rsync -avP root@l-sclt1.ops.cn2.qunar.com::CN2_MFS_DB_PG_TKT2/2017-12-20.key . rsync -avP root@l-sclt1.ops.cn2.qunar.com::CN2_MFS_DB_PG_TKT2/2017-12-20.tar.e . ll -h  

#### step 2: 将加密的备份文件解密，并截图
cat 2017-12-20.tar.e | ./qsec.sh -d ticket2_aes256.pem  2017-12-20.key > 2017-12-20.tar ll -h 
#### step 3: 解压备份文件 
ll -h  
tar -xvf 2017-12-20.tar 2017-12-20/l-pgdb7.tkt.cn5-*.tar.gz  
cd 2017-12-20 
ll -h  
tar -zxvf l-pgdb7.tkt.cn5-data-2017-12-20.tar.gz &>/dev/null tar -zxvf l-pgdb7.tkt.cn5-xlog-2017-12-20.tar.gz &>/dev/null ll -h  du -sh pg940_data
```

step 4：启动一个 pg 实例进行恢复

step 5：验证恢复



## 一些经验

我们知道，DB备份主要到过程如下：

1）主库用 archive_command 将 WAL 发送到从库所在的 Server 上

2）部署在从库所在 Server 上的DB备份任务脚本执行以下 3 步:

2.1）制作当天的DB基础备份

2.2）将昨天的DB基础备份与WAL打包并加密

2.3）将加密的包发送到远端存储集群



本人从开始工作到现在，常见的问题就下列几个：

1）主库向从库所在机器发送 WAL 失败

因为我们使用的 omni-pitr 工具通过 rsync 命令发送 WAL 。

而 rsync 接受端(即从库所在机器)需要将 rsync 发送端(即主库所在机器) 的 IP或hostname 配置在 IP 白名单中，

主库才能将 WAL 成功发送到从库所在机器上。

但有时因为 DBA 同学的大意，白名单未成功配置，就会导致主库向从库所在机器发送 WAL 失败。

2）制作基础备份失败

**a）白名单问题**

DB备份任务的脚本被部署在从库所在机器上，脚本里调用 omni-pitr 工具制作基础备份，

而 omni-pitr 制作基础备份时是通过 pgdba 用户连接主库来执行 pg_start_backup() 的，这就需要主库在 pg_hba.conf 中为从库机器配置 pgdba 的 replication 权限。

但有时因为 DBA 同学的大意，白名单未成功配置，就会导致制作基础备份失败。

**b）上一次备份任务还未结束**

上一次任务还未结束，本次任务会因为检测到 pid 文件还存在，而导致程序退出。



3）主从库流复制断了



不常见的问题有：

4）修改了 pitr_backup.sh 脚本做临时处理，但后来忘改回来

5）机器内核BUG

## 常见报错的可能原因及处理方案

1、备份脚本运行失败

2、制作基础备份失败

3、昨天的 basebackup 命令还在运行



## 命令

#### <1> 实际步骤

1. 制作基础备份

2. 创建 YYYY-MM-DD(昨天的日期) 目录

3. 将昨天0-24点产生的WAL文件从 realtime 目录移动到 YYYY-MM-DD 目录中

5. 将 YYYY-MM-DD 目录打包

6. 将打好的包加密，生成加密文件

7. 将 YYYY-MM-DD.key、YYYY-MM-DD.tar.e 传送到远程

8. 删除 YYYY-MM-DD 目录

9. 删除本地 YYYY-MM-DD.key、YYYY-MM-DD.tar.e、YYYY-MM-DD.tar 文件



#### <2> 常手动操作的步骤

3-8



#### <3> 工具步骤

```
S1 = 将指定日期 0-24点产生的 WAL 的gz文件从 realtime 目录移动到 YYYY-MM-DD 目录中``==== ``find` `./ -``type` `f -newerct ``'2019-09-16'` `! -newerct ``'2019-09-17'` `-``exec` `mv` `{} ..``/2019-09-16` `\;` `S2 = 将 YYYY-MM-DD 目录打包``==== ``tar` `-cf 2019-09-16.``tar` `2019-09-16` `S3 = 删除 YYYY-MM-DD 目录``==== ``rm` `-r 2019-09-16` `S4 = 生成 加密文件 和 加密KEY文件``==== ``cat` `2019-09-16.``tar` `| .``/qsec``.sh -e ``/home/q/pgdba/conf/qmsg1_aes256``.pub 2019-09-16.key >2019-09-16.``tar``.e` `S5 = 删除 ``tar` `包``==== ``rm` `2019-09-16.``tar` `S6 = 将 YYYY-MM-DD.key、YYYY-MM-DD.``tar``.e 传送到远程``==== ` `S6 = 将 YYYY-MM-DD.key、YYYY-MM-DD.``tar``.e 删除``====` `param1: 是否删除 key、``tar``.e``param2: 是否删除 YYYY-MM-DD 目录``param3: 起始步骤``param4: 日期
```



## 有些时间点需要两份DB备份才能恢复到

当前的 DB 备份任务，是把一天的 基础备份 和 0点到24点的WAL 归档文件 作为一个DB备份。

基于的逻辑是：

基础备份 + 从做基础备份开始后的 24 小时的 WAL 归档文件

做为一个 DB 备份。

然后，每天定时做这步骤。这样，我们就可以恢复到任何时间点。



那基础备份最好是在 0 点做，这样我们可以用 pitr 恢复到今天的任意一个时间点。

但实际上，大多 DB 备份任务不是在 0 点运行的，这样就导致一个问题：

假如我想恢复DB到 6月20号 0 点 到 DB基本备份制作时间点之间的一个时间点，

我就需要 6月19号 和 6月20号 的DB备份文件，因为 6月20号 0 点 到 DB基本备份制作时间点之间的 WAL 归档文件，





## 磁盘空间占用问题

当DB实例数据量较大时，DB备份也会较大，而又存储多天的DB备份文件时，会引起磁盘报警。

